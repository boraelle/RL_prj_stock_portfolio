"""
Main Training Script for RL Portfolio Management (ìµœì¢… ë²„ì „)
- ë°ì´í„°: 2024ë…„ 1ì›” ~ 2025ë…„ 11ì›” (23ê°œì›”)
- í•™ìŠµ ë° í‰ê°€: ì „ì²´ ë°ì´í„° ì‚¬ìš©
- ë¯¸ë˜ í…ŒìŠ¤íŠ¸: 2025ë…„ 12ì›” (ì œì¶œ í›„ ë³„ë„ í‰ê°€)
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Stable-Baselines3
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from environment import SectorPortfolioEnv
from models import create_ppo_policy_kwargs
from utils import calculate_metrics, plot_portfolio_performance, save_results


def load_data(data_path: str = 'data/etf_data_full.csv'):
    """
    ì „ì²´ ETF ë°ì´í„° ë¡œë“œ (2024.01 ~ 2025.11)
    """
    if not os.path.exists(data_path):
        raise FileNotFoundError(
            f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}\n"
            f"ë¨¼ì € 'python src/collect_data.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!"
        )
    
    df = pd.read_csv(data_path)
    df['Date'] = pd.to_datetime(df['Date'])
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë ˆì½”ë“œ")
    print(f"   ê¸°ê°„: {df['Date'].min().date()} ~ {df['Date'].max().date()}")
    print(f"   ê±°ë˜ì¼ ìˆ˜: {df['Date'].nunique()}ì¼ (ì•½ {df['Date'].nunique()/20:.1f}ê°œì›”)")
    print(f"   ì¢…ëª© ìˆ˜: {df['Ticker'].nunique()}ê°œ")
    
    return df


def create_env(df: pd.DataFrame, **kwargs):
    """
    ê°•í™”í•™ìŠµ í™˜ê²½ ìƒì„± (Vectorized)
    """
    def _init():
        return SectorPortfolioEnv(df, **kwargs)
    
    env = DummyVecEnv([_init])
    print(f"âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ")
    print(f"   Observation Space: Dict with {len(env.observation_space.spaces)} modals")
    print(f"   Action Space: {env.action_space.shape}")
    print(f"   Episode Length: {df['Date'].nunique() - kwargs.get('n_history', 20)}ì¼")
    
    return env


def train_agent(
    env,
    total_timesteps: int = 100_000,
    learning_rate: float = 3e-4,
    n_steps: int = 2048,
    batch_size: int = 64,
    n_epochs: int = 10,
    gamma: float = 0.99,
    save_dir: str = 'results'
):
    """
    PPO ì—ì´ì „íŠ¸ í•™ìŠµ
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # 1. Policy kwargs (LSTM Feature Extractor)
    policy_kwargs = create_ppo_policy_kwargs(env.observation_space)
    
    # 2. PPO ëª¨ë¸ ìƒì„±
    print("\nğŸš€ PPO ëª¨ë¸ ìƒì„± ì¤‘...")
    model = PPO(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=gamma,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        policy_kwargs=policy_kwargs,
        verbose=1,
        tensorboard_log=os.path.join(save_dir, 'tensorboard')
    )
    
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.policy.parameters()):,}ê°œ")
    
    # 3. Callbacks ì„¤ì •
    checkpoint_callback = CheckpointCallback(
        save_freq=10_000,
        save_path=os.path.join(save_dir, 'checkpoints'),
        name_prefix='ppo_portfolio'
    )
    
    # 4. í•™ìŠµ ì‹œì‘
    print(f"\nğŸ“ í•™ìŠµ ì‹œì‘ (ì´ {total_timesteps:,} timesteps)...")
    print(f"   ë°ì´í„°: 2024.01 ~ 2025.11 (23ê°œì›”)")
    print(f"   ì˜ˆìƒ ì‹œê°„: {(total_timesteps / 1000):.0f}ë¶„ (Mac CPU ê¸°ì¤€)")
    print(f"   TensorBoard: tensorboard --logdir {os.path.join(save_dir, 'tensorboard')}\n")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[checkpoint_callback],
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâš ï¸ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤...")
    
    # 5. ìµœì¢… ëª¨ë¸ ì €ì¥
    model_path = os.path.join(save_dir, 'ppo_portfolio_final.zip')
    model.save(model_path)
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: {model_path}")
    
    return model


def evaluate_agent(model, env, n_eval_episodes: int = 5):
    """
    í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€ (ì „ì²´ ë°ì´í„°)
    """
    print(f"\nğŸ“Š ì—ì´ì „íŠ¸ í‰ê°€ ì¤‘ ({n_eval_episodes} ì—í”¼ì†Œë“œ)...")
    
    all_capital_histories = []
    all_metrics = []
    
    for ep in range(n_eval_episodes):
        obs = env.reset()
        done = False
        capital_history = []
        
        while not done:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            
            # infoëŠ” VecEnvì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            if isinstance(info, list):
                info = info[0]
            
            capital_history.append(info['capital'])
            
            if done:
                break
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        initial_capital = capital_history[0] / (1 + info['total_return'])
        metrics = calculate_metrics(capital_history, initial_capital)
        
        all_capital_histories.append(capital_history)
        all_metrics.append(metrics)
        
        print(f"  Episode {ep+1}: Return={metrics['total_return']:.2f}%, "
              f"Sharpe={metrics['sharpe_ratio']:.2f}, MDD={metrics['max_drawdown']:.2f}%")
    
    # í‰ê·  ë©”íŠ¸ë¦­
    avg_metrics = {
        key: np.mean([m[key] for m in all_metrics])
        for key in all_metrics[0].keys()
    }
    
    print(f"\nâœ… í‰ê°€ ì™„ë£Œ (í‰ê· ):")
    print(f"   Total Return: {avg_metrics['total_return']:.2f}%")
    print(f"   Annualized Return: {avg_metrics['annualized_return']:.2f}%")
    print(f"   Sharpe Ratio: {avg_metrics['sharpe_ratio']:.2f}")
    print(f"   Max Drawdown: {avg_metrics['max_drawdown']:.2f}%")
    
    return {
        'metrics': avg_metrics,
        'capital_histories': all_capital_histories,
        'all_metrics': all_metrics
    }


def baseline_strategy(env, strategy='equal_weight'):
    """
    ë² ì´ìŠ¤ë¼ì¸ ì „ëµ í‰ê°€ (ë¹„êµìš©)
    """
    print(f"\nğŸ” ë² ì´ìŠ¤ë¼ì¸ ì „ëµ í‰ê°€: {strategy}")
    
    obs = env.reset()
    done = False
    capital_history = []
    
    n_assets = env.action_space.shape[0]
    
    if strategy == 'equal_weight':
        action = np.ones(n_assets) / n_assets
    elif strategy == 'buy_and_hold':
        action = np.ones(n_assets) / n_assets
        rebalance = True
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    while not done:
        if strategy == 'buy_and_hold' and not rebalance:
            action = env.get_attr('portfolio_weights')[0]
        
        obs, reward, done, info = env.step([action])
        
        if isinstance(info, list):
            info = info[0]
        
        capital_history.append(info['capital'])
        
        if strategy == 'buy_and_hold':
            rebalance = False
        
        if done:
            break
    
    # ë©”íŠ¸ë¦­
    initial_capital = capital_history[0] / (1 + info['total_return'])
    metrics = calculate_metrics(capital_history, initial_capital)
    
    print(f"  Return: {metrics['total_return']:.2f}%, "
          f"Sharpe: {metrics['sharpe_ratio']:.2f}, "
          f"MDD: {metrics['max_drawdown']:.2f}%")
    
    return capital_history


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 70)
    print("ğŸ¯ RL Portfolio Management Training")
    print("   Algorithm: PPO + LSTM")
    print("   Assets: 14 ETFs + Cash")
    print("   Period: 2024ë…„ 1ì›” ~ 2025ë…„ 11ì›” (23ê°œì›”)")
    print("=" * 70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_data('data/etf_data_full.csv')
    
    # 2. í™˜ê²½ ìƒì„±
    env = create_env(
        df,
        initial_capital=100_000_000,  # 1ì–µì›
        transaction_cost=0.0015,      # 0.15%
        n_history=20,
        cash_return=0.02,
        rebalance_freq=1
    )
    
    # 3. ì—ì´ì „íŠ¸ í•™ìŠµ
    model = train_agent(
        env,
        total_timesteps=150_000,  # 23ê°œì›” ë°ì´í„°ì´ë¯€ë¡œ ì¦ê°€
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        save_dir='results'
    )
    
    # 4. í‰ê°€
    eval_results = evaluate_agent(model, env, n_eval_episodes=3)
    
    # 5. ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ
    baseline_equal = baseline_strategy(env, strategy='equal_weight')
    baseline_bh = baseline_strategy(env, strategy='buy_and_hold')
    
    # 6. ì‹œê°í™”
    print("\nğŸ“Š ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    initial_capital = 100_000_000
    best_capital = eval_results['capital_histories'][0]
    
    plot_portfolio_performance(
        best_capital,
        initial_capital,
        save_path='results/rl_performance.png'
    )
    
    # ì „ëµ ë¹„êµ
    from utils import compare_strategies
    compare_strategies(
        {
            'RL Agent (PPO+LSTM)': best_capital,
            'Equal Weight': baseline_equal,
            'Buy & Hold': baseline_bh
        },
        initial_capital,
        save_path='results/strategy_comparison.png'
    )
    
    # 7. ê²°ê³¼ ì €ì¥
    save_results(
        eval_results['metrics'],
        best_capital,
        output_dir='results'
    )
    
    print("\n" + "=" * 70)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“ ê²°ê³¼ë¬¼:")
    print(f"   - ëª¨ë¸: results/ppo_portfolio_final.zip")
    print(f"   - ì„±ê³¼ ì°¨íŠ¸: results/rl_performance.png")
    print(f"   - ì „ëµ ë¹„êµ: results/strategy_comparison.png")
    print(f"   - ë©”íŠ¸ë¦­: results/metrics.csv")
    print(f"   - ìë³¸ ì´ë ¥: results/capital_history.csv")
    
    print(f"\nğŸ’¡ TensorBoard í™•ì¸:")
    print(f"   tensorboard --logdir results/tensorboard")
    
    print(f"\nğŸ”® ë¯¸ë˜ í…ŒìŠ¤íŠ¸ (ì œì¶œ í›„):")
    print(f"   2025ë…„ 12ì›” ë°ì´í„°ë¡œ ìµœì¢… ê²€ì¦:")
    print(f"   1. python src/collect_december.py (12ì›” í›„)")
    print(f"   2. python src/evaluate_december.py")
    
    print(f"\nğŸ“Š í”„ë¡œì íŠ¸ ì„±ê³¼ ìš”ì•½:")
    print(f"   ê¸°ê°„: 2024.01 ~ 2025.11 (23ê°œì›”)")
    print(f"   ì´ ìˆ˜ìµë¥ : {eval_results['metrics']['total_return']:.2f}%")
    print(f"   ì—°í™˜ì‚° ìˆ˜ìµë¥ : {eval_results['metrics']['annualized_return']:.2f}%")
    print(f"   Sharpe Ratio: {eval_results['metrics']['sharpe_ratio']:.2f}")
    print(f"   Max Drawdown: {eval_results['metrics']['max_drawdown']:.2f}%")
    

if __name__ == "__main__":
    main()
